{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyM+z5HMZE12Yt8gjoigPxtQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["1. Mount Google Drive"],"metadata":{"id":"vcrihJ-qBNvm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rj-fe0PPZxJx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762533622467,"user_tz":360,"elapsed":17825,"user":{"displayName":"Sanaz Rabinia","userId":"18423678739563779329"}},"outputId":"cdd617f4-b15c-4b34-ab0f-b5f7c30ec62d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["2. Define project paths"],"metadata":{"id":"WsILFvO_BSdQ"}},{"cell_type":"code","source":["BASE_DIR = '/content/drive/MyDrive/CSAFE_Handwriting'\n","DATA_DIR = f'{BASE_DIR}/data/writers'\n","SPLITS_DIR = f'{BASE_DIR}/splits'\n","CKPT_DIR = f'{BASE_DIR}/checkpoints'\n","SRC_DIR = f'{BASE_DIR}/src'\n","\n","import os\n","for path in [DATA_DIR, SPLITS_DIR, SRC_DIR, CKPT_DIR]:\n","    print(path, 'correct' if os.path.exists(path) else 'wrong')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-WvLQQ_iBLWB","executionInfo":{"status":"ok","timestamp":1762533642255,"user_tz":360,"elapsed":1107,"user":{"displayName":"Sanaz Rabinia","userId":"18423678739563779329"}},"outputId":"59c9d3f4-42cd-4b3e-cea5-a7ccc2982260"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/CSAFE_Handwriting/data/writers correct\n","/content/drive/MyDrive/CSAFE_Handwriting/splits correct\n","/content/drive/MyDrive/CSAFE_Handwriting/src correct\n","/content/drive/MyDrive/CSAFE_Handwriting/checkpoints correct\n"]}]},{"cell_type":"markdown","source":["3. Copy data to fast disk (/content)\n","\n","This is for reading many PNGs from Drive.\n"],"metadata":{"id":"15VsXRmNBcNY"}},{"cell_type":"code","source":["!mkdir -p /content/data/writers\n","!mkdir -p /content/splits"],"metadata":{"id":"O--EKfFSB4Tt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!rsync -a --info=progress2 \"$DATA_DIR/\" \"/content/data/writers/\"\n","!rsync -a \"$SPLITS_DIR/\" \"/content/splits/\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v_M3t0DCBpD6","outputId":"6b7711b0-3b39-4e18-b498-bbb71b34c0c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  4,147,064,358  92%  597.43kB/s    1:52:58 (xfr#11261, ir-chk=1051/12788)"]}]},{"cell_type":"code","source":["import os\n","print(\"Writer directories:\", len(os.listdir(\"/content/data/writers\")))\n","print(\"Split files:\", os.listdir(\"/content/splits\"))"],"metadata":{"id":"yamSauI1Ak9V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","SRC_DIR = \"/content/drive/MyDrive/CSAFE_Handwriting/src\"\n","\n","print(\"SRC_DIR:\", SRC_DIR)\n","files = os.listdir(SRC_DIR)\n","for f in files:\n","    print(\" -\", f)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MQ0SQt2MrRgG","executionInfo":{"status":"ok","timestamp":1762457887656,"user_tz":360,"elapsed":9,"user":{"displayName":"Sanaz Rabinia","userId":"18423678739563779329"}},"outputId":"d40e6dbf-b946-4805-b1cb-8ab4b6b11013"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SRC_DIR: /content/drive/MyDrive/CSAFE_Handwriting/src\n"," - train_baseline.py\n"]}]},{"cell_type":"code","source":["import os, re, json, unicodedata\n","from pathlib import Path\n","\n","WRITERS_ROOT = Path(\"/content/data/writers\")\n","SPLITS_DIR   = Path(\"/content/splits\")\n","\n","# helper\n","def norm(s):\n","    # normalize unicode, strip whitespace (incl. newlines, tabs), collapse spaces\n","    s2 = unicodedata.normalize(\"NFKC\", s)\n","    s2 = s2.strip()\n","    s2 = re.sub(r\"\\s+\", \"\", s2)\n","    return s2\n","\n","# list directories (repr shows hidden whitespace/newlines)\n","dirs = [d for d in os.listdir(WRITERS_ROOT) if (WRITERS_ROOT/d).is_dir()]\n","print(\"Total dirs under writers:\", len(dirs))\n","\n","# identify suspicious names\n","valid_pat = re.compile(r\"^w\\d{4,}$\")  # e.g., w0001\n","suspect = []\n","for d in dirs:\n","    n = norm(d)\n","    if (not n) or (not valid_pat.match(n)):\n","        suspect.append((d, n, len(d)))\n","\n","print(\"\\nSUSPECT folders (showing up to 20):\")\n","for i, (raw, nrm, L) in enumerate(suspect[:20], 1):\n","    print(f\"{i:02d}) raw={repr(raw)} | normalized={repr(nrm)} | len={L}\")\n","\n","print(\"\\nCounts -> valid:\", len(dirs)-len(suspect), \"| suspect:\", len(suspect))\n","\n","# show split IDs and which won't match disk after normalization\n","def load_ids(name):\n","    with open(SPLITS_DIR/f\"{name}.json\",\"r\",encoding=\"utf-8\") as f:\n","        return json.load(f)\n","\n","train_ids = load_ids(\"train\")\n","val_ids   = load_ids(\"val\")\n","test_ids  = load_ids(\"test\")\n","\n","all_ids_raw = train_ids + val_ids + test_ids\n","all_ids_norm = [norm(x) for x in all_ids_raw]\n","\n","dir_set_norm = {norm(d) for d in dirs if valid_pat.match(norm(d))}\n","split_set_norm = set(all_ids_norm)\n","\n","missing_on_disk = sorted(split_set_norm - dir_set_norm)\n","extra_on_disk   = sorted(dir_set_norm - split_set_norm)\n","\n","print(\"\\nSplit sizes -> train/val/test:\", len(train_ids), len(val_ids), len(test_ids))\n","print(\"Unique split IDs (normalized):\", len(split_set_norm))\n","print(\"Missing on disk (first 20):\", missing_on_disk[:20])\n","print(\"Extra on disk (first 20):\", extra_on_disk[:20])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IFELjzYOsNbY","executionInfo":{"status":"ok","timestamp":1762458135408,"user_tz":360,"elapsed":18,"user":{"displayName":"Sanaz Rabinia","userId":"18423678739563779329"}},"outputId":"ab4efc48-0a97-480b-e89c-ec69485140c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total dirs under writers: 475\n","\n","SUSPECT folders (showing up to 20):\n","\n","Counts -> valid: 475 | suspect: 0\n","\n","Split sizes -> train/val/test: 1 1 1\n","Unique split IDs (normalized): 1\n","Missing on disk (first 20): ['']\n","Extra on disk (first 20): ['w0001', 'w0002', 'w0003', 'w0004', 'w0005', 'w0006', 'w0009', 'w0010', 'w0011', 'w0012', 'w0013', 'w0015', 'w0016', 'w0017', 'w0018', 'w0020', 'w0022', 'w0023', 'w0024', 'w0025']\n"]}]},{"cell_type":"code","source":["import os, re, json, random, shutil\n","from pathlib import Path\n","\n","# Paths\n","WRITERS_ROOT = Path(\"/content/data/writers\")\n","LOCAL_SPLITS = Path(\"/content/splits\")\n","DRIVE_SPLITS = Path(\"/content/drive/MyDrive/CSAFE_Handwriting/splits\")\n","\n","LOCAL_SPLITS.mkdir(parents=True, exist_ok=True)\n","DRIVE_SPLITS.mkdir(parents=True, exist_ok=True)\n","\n","# Backup any existing splits in Drive (simple copy with suffix)\n","for name in [\"train.json\", \"val.json\", \"test.json\"]:\n","    src = DRIVE_SPLITS / name\n","    if src.exists():\n","        shutil.copy2(src, DRIVE_SPLITS / f\"{name}.bak\")\n","\n","# Collect valid writer IDs\n","valid_pat = re.compile(r\"^w\\d{4,}$\")\n","writers = sorted([d.name for d in WRITERS_ROOT.iterdir() if d.is_dir() and valid_pat.match(d.name)])\n","\n","print(\"Detected writers:\", len(writers))\n","assert len(writers) > 3, \"Not enough writers found to split.\"\n","\n","# Reproducible shuffle\n","random.seed(42)\n","random.shuffle(writers)\n","\n","# Ratios\n","train_ratio, val_ratio, test_ratio = 0.70, 0.15, 0.15\n","N = len(writers)\n","n_train = int(N * train_ratio)\n","n_val   = int(N * val_ratio)\n","n_test  = N - n_train - n_val\n","\n","train_ids = writers[:n_train]\n","val_ids   = writers[n_train:n_train+n_val]\n","test_ids  = writers[n_train+n_val:]\n","\n","print(f\"Split sizes -> train={len(train_ids)} val={len(val_ids)} test={len(test_ids)} (total={N})\")\n","\n","# Save to LOCAL (used by training) and DRIVE (for persistence)\n","for target in [LOCAL_SPLITS, DRIVE_SPLITS]:\n","    with open(target/\"train.json\", \"w\", encoding=\"utf-8\") as f: json.dump(train_ids, f, indent=2)\n","    with open(target/\"val.json\",   \"w\", encoding=\"utf-8\") as f: json.dump(val_ids,   f, indent=2)\n","    with open(target/\"test.json\",  \"w\", encoding=\"utf-8\") as f: json.dump(test_ids,  f, indent=2)\n","\n","# Quick verify\n","def load_ids(p):\n","    with open(p,\"r\",encoding=\"utf-8\") as f: return json.load(f)\n","\n","lt, lv, ls = load_ids(LOCAL_SPLITS/\"train.json\"), load_ids(LOCAL_SPLITS/\"val.json\"), load_ids(LOCAL_SPLITS/\"test.json\")\n","print(\"LOCAL check:\", len(lt), len(lv), len(ls))\n","dt, dv, ds = load_ids(DRIVE_SPLITS/\"train.json\"), load_ids(DRIVE_SPLITS/\"val.json\"), load_ids(DRIVE_SPLITS/\"test.json\")\n","print(\"DRIVE check:\", len(dt), len(dv), len(ds))\n","\n","print(\"First few train IDs:\", lt[:5])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HYC-IBo7sfIj","executionInfo":{"status":"ok","timestamp":1762458206700,"user_tz":360,"elapsed":88,"user":{"displayName":"Sanaz Rabinia","userId":"18423678739563779329"}},"outputId":"2ca6cd59-86f5-482e-81b8-ea52a4f1725c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Detected writers: 475\n","Split sizes -> train=332 val=71 test=72 (total=475)\n","LOCAL check: 332 71 72\n","DRIVE check: 332 71 72\n","First few train IDs: ['w0352', 'w0289', 'w0064', 'w0077', 'w0518']\n"]}]},{"cell_type":"code","source":["SRC_DIR = \"/content/drive/MyDrive/CSAFE_Handwriting/src\"\n","print(\"SRC_DIR exists:\", SRC_DIR, os.path.exists(SRC_DIR))\n","print(\"Files currently inside src:\", os.listdir(SRC_DIR))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Upf1S0Uyeqb","executionInfo":{"status":"ok","timestamp":1762459775719,"user_tz":360,"elapsed":10,"user":{"displayName":"Sanaz Rabinia","userId":"18423678739563779329"}},"outputId":"45780520-763c-42f1-bd66-398a212df8d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SRC_DIR exists: /content/drive/MyDrive/CSAFE_Handwriting/src True\n","Files currently inside src: ['train_baseline.py', 'train_resnet.py']\n"]}]},{"cell_type":"code","source":["import re\n","\n","fp = \"/content/drive/MyDrive/CSAFE_Handwriting/src/train_resnet.py\"\n","\n","with open(fp,\"r\") as f:\n","    code = f.read()\n","\n","# Replace the scheduler line (remove verbose=True)\n","code = re.sub(\n","    r'scheduler\\s*=\\s*optim\\.lr_scheduler\\.ReduceLROnPlateau\\(optimizer, mode=\"max\", factor=0.5, patience=2, verbose=True\\)',\n","    'scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=2)',\n","    code\n",")\n","\n","with open(fp,\"w\") as f:\n","    f.write(code)\n","\n","print(\"✅ Patched successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"92f_2lqRy6Sm","executionInfo":{"status":"ok","timestamp":1762459889546,"user_tz":360,"elapsed":45,"user":{"displayName":"Sanaz Rabinia","userId":"18423678739563779329"}},"outputId":"ae258a75-44a5-47bf-922e-31a4cc4de8c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Patched successfully.\n"]}]},{"cell_type":"code","source":["import sys, runpy\n","SCRIPT = \"/content/drive/MyDrive/CSAFE_Handwriting/src/train_resnet_closedset.py\"\n","sys.argv = [\n","    \"train_resnet_closedset.py\",\n","    \"--data_root\", \"/content/data/writers\",\n","    \"--ckpt_dir\", \"/content/drive/MyDrive/CSAFE_Handwriting/checkpoints\",\n","    \"--image_size\", \"224\",\n","    \"--batch_size\", \"32\",\n","    \"--epochs\", \"10\",\n","    \"--lr\", \"1e-3\",\n","    \"--weight_decay\", \"1e-4\",\n","]\n","print(\"Running:\", SCRIPT)\n","print(\"Args:\", \" \".join(sys.argv[1:]))\n","runpy.run_path(SCRIPT, run_name=\"__main__\")"],"metadata":{"id":"HNQC5A1dMFkG"},"execution_count":null,"outputs":[]}]}